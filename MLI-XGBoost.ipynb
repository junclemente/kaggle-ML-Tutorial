{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Machine Learning - kaggle\n",
    "\n",
    "https://www.kaggle.com/learn/intermediate-machine-learning\n",
    "\n",
    "### XGBoost / Gradient Boosting\n",
    "\n",
    "https://www.kaggle.com/code/alexisbcook/xgboost/tutorial\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Previous predictions have been made with the random forest method. This achieves better peformance than a single decision tree simply by averaging the predictions of many decision trees. \n",
    "\n",
    "Random forest method is referred to as an \"ensemble\" method. **Ensemble methods** combine the predictions of severl models (e.g., several trees, in the case of random forests). Gradient boosting is another ensemble method. \n",
    "\n",
    "**** Gradient Boosting\n",
    "\n",
    "**Gradient boosting** is a method that goes through cycles to iteratively add models into an ensemble. \n",
    "\n",
    "Begin by initializing the ensemble with a single model. Then, start the cycle:\n",
    "\n",
    "- First, use the current ensemble to generate predictions for each observation in the dataset. To make a prediciton, we add the predictions from all modcels in the ensemble. \n",
    "- These predictions are used to calculate a loss function (like mean squared error)\n",
    "- Then, use the loss function to fig a new modle that will be added to the ensemble. We determine parameters so that adding this new modle to the ensemble will reduce the loss. \n",
    "    - The *\"gradient\"* in *\"gradient boosting\"* refers to the fact that we'll use gradient descent on the loss function to determine the parameters in this new model. \n",
    "- Finally, we add the new model to ensemble\n",
    "- Repeat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data \n",
    "data = pd.read_csv('./melbourne-housing-snapshot/melb_data.csv')\n",
    "\n",
    "# Select subset of predictors\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "\n",
    "# Select target\n",
    "y = data.Price\n",
    "\n",
    "# Separate data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science-Py3-10-13_20231128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
