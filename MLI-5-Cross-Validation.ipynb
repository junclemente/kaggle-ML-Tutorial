{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Machine Learning - kaggle\n",
    "\n",
    "https://www.kaggle.com/learn/intermediate-machine-learning\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "https://www.kaggle.com/code/alexisbcook/cross-validation\n",
    "\n",
    "**Cross-validation** is running the modeling process on different subsets of the data to get multiple measures of model quality. \n",
    "\n",
    "Start by dividing the data into 5 pieces, or **folds**, (20% of the full dataset). Then, run an experiment for each fold:\n",
    "- **Experiment 1**: use the first fold as a validation (or holdout) set and everything else as training data. This gives us a measure of model quality based on a 20% holdout set. \n",
    "- **Experiment 2**: hold out data from the second fold and use everything else for the training model. \n",
    "- The process is repeated using every fold once as the holdout set. Putting this together, 100% of the data is used as holdout, and we end up with a measure of model quality that is based on all of the rows in the dataset even though all rows are not used simultaneously. \n",
    "\n",
    "### When should cross-validation be used? \n",
    "\n",
    "Cross-validation gives a more accurate measure of model quality but it can take longer to run because it estimates multiple models\n",
    "\n",
    "- *For small datasets*, where extra computational burden isn't a big deal, you should run cross-validation.\n",
    "- *For larger datasets*, a single validation set is sufficient, Code will run faster, and you may have enough data that there's little need to re-use some of it for holdout. \n",
    "\n",
    "There's no threshold for what is a large vs small dataset. If your model takes a couple minutes or less to run, it could be worth switching to cross-valdation. \n",
    "\n",
    "Also, when running cross-validation and the scores for each experiment seem to be the same results, a single validation set is probably sufficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('./melbourne-housing-snapshot/melb_data.csv')\n",
    "\n",
    "# Select subset of predictors\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "\n",
    "# Select target\n",
    "y = data.Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a pipeline that uses an imputer to fill in missing values and a random forest model to make predictions. \n",
    "\n",
    "Cross-validation can be done without pipelines but it makes it more difficult. Using a pipeline will make the code straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n",
    "                              ('model', RandomForestRegressor(n_estimators=50, random_state=0))\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation scores are obtained with the `cross_val_score()` function from scikit-learn. Set the number of folds with the `cv` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE scores:\n",
      " [301628.7893587  303164.4782723  287298.331666   236061.84754543\n",
      " 260383.45111427]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5, \n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"MAE scores:\\n\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scoring` parameter chooses a measure of model quality: `neg_mean_absolute_error`. \n",
    "A list of options can be found in the [docs for sci-kit learn](https://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "\n",
    "Scikit-learn has a convention where all metrics are defined so a high number is better. That is why a negative MAE is specified. \n",
    "\n",
    "Typically, we want a single measure of model quality to compare alternative models. So an average is taken across experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE score (across experiments): \n",
      "277707.3795913405\n"
     ]
    }
   ],
   "source": [
    "print(\"Average MAE score (across experiments): \")\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Using cross-validation yields a better measure of model quality. Note that we no longer have to keep track of separate training and validation sets. So, for small datasets, it's a good improvement. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science-Py3-10-13_20231128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
